{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import mimetypes\n",
    "import unicodedata\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchField, ComplexField,\n",
    "    SearchFieldDataType, SemanticConfiguration, SemanticField,\n",
    "    SemanticPrioritizedFields, SemanticSearch, VectorSearch,\n",
    "    VectorSearchProfile, HnswAlgorithmConfiguration\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_FORM_RECOGNIZER_ENDPOINT = os.getenv(\"AZURE_FORM_RECOGNIZER_ENDPOINT\")\n",
    "AZURE_FORM_RECOGNIZER_API_KEY = os.getenv(\"AZURE_FORM_RECOGNIZER_API_KEY\")\n",
    "INTERMEDIATE_STEPS_PATH = \"/app/index_data/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 3: Procesar documentos usando Azure Form Recognizer\n",
    "def process_documents(input_path, output_path):\n",
    "    document_files = [f for f in os.listdir(input_path) if f.endswith((\".jpg\", \".png\", \".pptx\", \".pdf\", \".txt\", \".bmp\", \".tiff\"))]\n",
    "    form_recognizer_client = DocumentAnalysisClient(endpoint=AZURE_FORM_RECOGNIZER_ENDPOINT, credential=AzureKeyCredential(AZURE_FORM_RECOGNIZER_API_KEY))\n",
    "\n",
    "    # Configuración de TokenTextSplitter\n",
    "    text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    for document_file in document_files:\n",
    "        file_path = os.path.join(input_path, document_file)\n",
    "\n",
    "        # Validar formato MIME del archivo\n",
    "        mime_type, _ = mimetypes.guess_type(file_path)\n",
    "        try:\n",
    "            if mime_type in [\"application/pdf\", \"image/jpeg\", \"image/png\", \"image/bmp\", \"image/tiff\"]:\n",
    "                # Procesar con Azure Form Recognizer\n",
    "                with open(file_path, \"rb\") as file:\n",
    "                    poller = form_recognizer_client.begin_analyze_document(\"prebuilt-read\", document=file)\n",
    "                    result = poller.result()\n",
    "\n",
    "                # Extraer texto\n",
    "                text_content = \" \".join([line.content for page in result.pages for line in page.lines])\n",
    "\n",
    "            elif mime_type == \"text/plain\":\n",
    "                # Leer archivos .txt manualmente\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text_content = file.read()\n",
    "\n",
    "            # Normalizar texto\n",
    "            text_content_normalized = unicodedata.normalize(\"NFC\", text_content)\n",
    "\n",
    "            # Realizar chunking con TokenTextSplitter\n",
    "            chunks = text_splitter.split_text(text_content_normalized)\n",
    "\n",
    "            # Generar metadatos para cada chunk\n",
    "            chunked_data = []\n",
    "            for chunk in chunks:\n",
    "                metadata = {\n",
    "                    \"count_tokens\": len(chunk.split()),\n",
    "                    \"count_characters\": len(chunk),\n",
    "                    \"source\": document_file,\n",
    "                    \"type_source\": mime_type\n",
    "                }\n",
    "                chunked_data.append({\"content\": chunk, \"metadata\": metadata})\n",
    "\n",
    "            # Guardar chunks en un único archivo JSON\n",
    "            output_file = os.path.join(output_path, f\"{os.path.splitext(document_file)[0]}.json\")\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(chunked_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {document_file}: {e}\")\n",
    "\n",
    "process_documents(\"/app/index_data/data\",\"/app/index_data/data_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_azure_openai():\n",
    "    \"\"\"\n",
    "    Configura el cliente de Azure OpenAI utilizando las variables de entorno.\n",
    "    \"\"\"\n",
    "    return AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "def generate_hash_id(content):\n",
    "    \"\"\"\n",
    "    Genera un ID hash único para un chunk basado en su contenido.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando hash ID: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def preprocess_content(content):\n",
    "    \"\"\"\n",
    "    Limpia y preprocesa el contenido para evitar activar el filtro de contenido.\n",
    "    \"\"\"\n",
    "    content = re.sub(r\"http\\S+|www\\S+|@\\S+\", \"\", content)\n",
    "    content = re.sub(r\"[^\\w\\s]\", \"\", content)\n",
    "    return content[:4000]\n",
    "\n",
    "def save_data_as_json(data, output_file):\n",
    "    \"\"\"\n",
    "    Guarda los datos en un archivo JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar el archivo JSON: {e}\")\n",
    "\n",
    "def process_entry(entry, client, combined_data, json_file):\n",
    "    \"\"\"\n",
    "    Procesa una sola entrada JSON para generar palabras clave y embeddings.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    if \"content\" not in entry or \"metadata\" not in entry:\n",
    "        raise ValueError(f\"El archivo {json_file} no tiene los campos necesarios.\")\n",
    "\n",
    "    if not entry[\"content\"].strip():\n",
    "        error_msg = f\"Contenido vacío en {json_file}.\"\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    entry[\"content\"] = preprocess_content(entry[\"content\"])\n",
    "\n",
    "    try:\n",
    "        current_id = generate_hash_id(entry[\"content\"])\n",
    "    except ValueError as ve:\n",
    "        errors.append(f\"Error generando hash ID para {json_file}: {ve}\")\n",
    "        return errors\n",
    "\n",
    "    if \"keywords\" not in entry or not entry[\"keywords\"]:\n",
    "        try:\n",
    "            prompt = f\"Extrae las 5 palabras clave más importantes del siguiente texto: {entry['content']}\"\n",
    "            keywords_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Eres un modelo que genera palabras clave.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=50,\n",
    "            )\n",
    "            if keywords_response.choices and keywords_response.choices[0].message.content:\n",
    "                keywords_text = keywords_response.choices[0].message.content.strip()\n",
    "\n",
    "                # Usamos regex para extraer palabras clave eliminando numeración y saltos de línea\n",
    "                extracted_keywords = re.findall(r\"[\\d\\.\\-\\) ]*(.*)\", keywords_text)\n",
    "\n",
    "                # Limpiamos los elementos extraídos eliminando espacios vacíos\n",
    "                entry[\"keywords\"] = [kw.strip() for kw in extracted_keywords if kw.strip()]\n",
    "            else:\n",
    "                logging.warning(f\"Respuesta filtrada o vacía para keywords en {json_file}. Respuesta: {keywords_response}\")\n",
    "                entry[\"keywords\"] = [\"content_filtered\"]\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error generando keywords para {json_file}: {e}\")\n",
    "            entry[\"keywords\"] = [\"error_generating_keywords\"]\n",
    "\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=entry[\"content\"],\n",
    "            model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\", \"text-embedding-ada-002\")\n",
    "        )\n",
    "        entry[\"embeddings\"] = response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Error generando embeddings para {json_file}: {e}\")\n",
    "        entry[\"embeddings\"] = []\n",
    "\n",
    "    combined_data.append({\n",
    "        \"id\": current_id,\n",
    "        \"content\": entry[\"content\"],\n",
    "        \"metadata\": entry[\"metadata\"],\n",
    "        \"keywords\": entry[\"keywords\"],\n",
    "        \"embeddings\": entry[\"embeddings\"]\n",
    "    })\n",
    "\n",
    "    return errors\n",
    "\n",
    "def generate_embeddings_and_keywords(client, input_paths, output_path):\n",
    "    \"\"\"\n",
    "    Procesa archivos JSON y genera embeddings y palabras clave.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    combined_data = []\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        json_files = [f for f in os.listdir(input_path) if f.endswith(\".json\")]\n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(input_path, json_file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                if isinstance(data, list):\n",
    "                    for entry in data:\n",
    "                        process_entry(entry, client, combined_data, json_file)\n",
    "                elif isinstance(data, dict):\n",
    "                    process_entry(data, client, combined_data, json_file)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error procesando {json_file}: {e}\")\n",
    "\n",
    "    output_json_file = os.path.join(output_path, \"step5_combined_2.json\")\n",
    "    save_data_as_json(combined_data, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = configure_azure_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando generación de embeddings y palabras clave...\n"
     ]
    }
   ],
   "source": [
    "input_paths = [\n",
    "    \"/app/index_data/data_processed\",\n",
    "]\n",
    "output_path = \"/app/index_data/data_processed_embeddings\"\n",
    "\n",
    "print(\"Iniciando generación de embeddings y palabras clave...\")\n",
    "generate_embeddings_and_keywords(client, input_paths, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Función para crear o actualizar un índice en Azure AI Search\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_or_update_index(index_name, search_endpoint, search_api_key):\n",
    "    \"\"\"Crea o actualiza un índice en Azure AI Search con configuración de búsqueda vectorial y semántica.\"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(search_api_key)\n",
    "        index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
    "\n",
    "        fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "            SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True, analyzer_name=\"standard.lucene\"),\n",
    "            ComplexField(name=\"metadata\", fields=[\n",
    "                SearchField(name=\"category\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "                SimpleField(name=\"count_characters\", type=SearchFieldDataType.Int32, filterable=True, sortable=True),\n",
    "                SimpleField(name=\"count_tokens\", type=SearchFieldDataType.Int32, filterable=True, sortable=True),\n",
    "                SearchField(name=\"source\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SearchField(name=\"type_source\", type=SearchFieldDataType.String, filterable=True),\n",
    "            ]),\n",
    "            SearchField(name=\"keywords\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), searchable=True),\n",
    "            SearchField(\n",
    "                name=\"content_vector\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                vector_search_dimensions=3072,\n",
    "                searchable=True,\n",
    "                vector_search_profile_name=\"myHnswProfile\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        semantic_config = SemanticConfiguration(\n",
    "            name=\"my-semantic-config\",\n",
    "            prioritized_fields=SemanticPrioritizedFields(\n",
    "                title_field=SemanticField(field_name=\"metadata/source\"),\n",
    "                keywords_fields=[SemanticField(field_name=\"keywords\")],\n",
    "                content_fields=[SemanticField(field_name=\"content\")]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=VectorSearch(\n",
    "                algorithms=[HnswAlgorithmConfiguration(name=\"myHnsw\")],\n",
    "                profiles=[VectorSearchProfile(name=\"myHnswProfile\", algorithm_configuration_name=\"myHnsw\")]\n",
    "            ),\n",
    "            semantic_search=SemanticSearch(configurations=[semantic_config])\n",
    "        )\n",
    "\n",
    "        index_client.create_or_update_index(index)\n",
    "        logging.info(f\"Índice '{index_name}' creado o actualizado exitosamente.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al crear/actualizar el índice: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_api_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "index_name = \"index-chatbot-in-a-day\" # Reemplaza con tu indice\n",
    "\n",
    "create_or_update_index(index_name, search_endpoint, search_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Función para obtener IDs de documentos en Azure AI Search\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_existing_chunk_ids(search_endpoint, index_name, search_api_key):\n",
    "    \"\"\"Obtiene los IDs de los chunks ya almacenados en Azure Search.\"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(search_api_key)\n",
    "        search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "        results = search_client.search(search_text=\"*\", select=[\"id\"])\n",
    "        existing_ids = [result[\"id\"] for result in results]\n",
    "        return existing_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error obteniendo IDs existentes de Azure Search: {e}\", exc_info=True)\n",
    "        raise ValueError(f\"Error obteniendo IDs existentes de Azure Search: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Función para eliminar documentos obsoletos en Azure AI Search\n",
    "# ------------------------------------------------------------------------------\n",
    "def delete_obsolete_chunks(search_endpoint, index_name, search_api_key, existing_ids, new_ids):\n",
    "    \"\"\"Elimina los documentos en Azure Search que están en 'existing_ids' pero no en 'new_ids'.\"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(search_api_key)\n",
    "        search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "        chunks_to_delete = set(existing_ids) - set(new_ids)\n",
    "        logging.info(f\"Chunks a eliminar: {chunks_to_delete}\")\n",
    "\n",
    "        if chunks_to_delete:\n",
    "            documents_to_delete = [{\"id\": doc_id} for doc_id in chunks_to_delete]\n",
    "            try:\n",
    "                response = search_client.delete_documents(documents=documents_to_delete)\n",
    "                for res in response:\n",
    "                    if res.succeeded:\n",
    "                        logging.info(f\"Documento eliminado exitosamente: {res.key}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Fallo al eliminar el documento {res.key}: {res.error_message}\")\n",
    "            except Exception as inner_e:\n",
    "                logging.error(f\"Error eliminando documentos: {inner_e}\", exc_info=True)\n",
    "        else:\n",
    "            logging.info(\"No hay documentos obsoletos para eliminar.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error eliminando chunks obsoletos de Azure Search: {e}\", exc_info=True)\n",
    "        raise ValueError(f\"Error eliminando chunks obsoletos de Azure Search: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Función para subir documentos a Azure AI Search\n",
    "# ------------------------------------------------------------------------------\n",
    "def upload_to_azure_search(datos, index_name, search_endpoint, search_api_key):\n",
    "    \"\"\"Carga documentos a Azure AI Search y elimina documentos obsoletos.\"\"\"\n",
    "    errores = []\n",
    "\n",
    "    if not create_or_update_index(index_name, search_endpoint, search_api_key):\n",
    "        logging.error(\"No se pudo crear o actualizar el índice.\")\n",
    "        return [\"No se pudo crear el índice.\"]\n",
    "\n",
    "    credential = AzureKeyCredential(search_api_key)\n",
    "    search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    # Preparar documentos para la carga\n",
    "    documentos = []\n",
    "    for item in datos:\n",
    "        metadata = item.get(\"metadata\", {})\n",
    "        if not isinstance(metadata, dict):\n",
    "            logging.warning(f\"El campo 'metadata' no es un diccionario para el ID: {item.get('id')}.\")\n",
    "            metadata = {}\n",
    "\n",
    "        documentos.append({\n",
    "            \"id\": str(item.get(\"id\", \"\")),\n",
    "            \"content\": item.get(\"content\", \"\"),\n",
    "            \"metadata\": {\n",
    "                \"category\": metadata.get(\"category\", \"\"),\n",
    "                \"count_characters\": metadata.get(\"count_characters\", 0),\n",
    "                \"count_tokens\": metadata.get(\"count_tokens\", 0),\n",
    "                \"source\": metadata.get(\"source\", \"\"),\n",
    "                \"type_source\": metadata.get(\"type_source\", \"\")\n",
    "            },\n",
    "            \"keywords\": [str(p) for p in item.get(\"keywords\", [])],\n",
    "            \"content_vector\": item.get(\"embeddings\", [])\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Documentos a cargar: {len(documentos)}\")\n",
    "\n",
    "    existing_ids = set(get_existing_chunk_ids(search_endpoint, index_name, search_api_key))\n",
    "    new_ids = set(doc[\"id\"] for doc in documentos)\n",
    "\n",
    "    delete_obsolete_chunks(search_endpoint, index_name, search_api_key, existing_ids, new_ids)\n",
    "\n",
    "    # Subir documentos en lotes de 50\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(documentos), batch_size):\n",
    "        batch = documentos[i:i+batch_size]\n",
    "        try:\n",
    "            response = search_client.upload_documents(documents=batch)\n",
    "            if all(res.succeeded for res in response):\n",
    "                logging.info(f\"Lote {i}-{i+batch_size}: {len(batch)} documentos subidos exitosamente.\")\n",
    "            else:\n",
    "                logging.error(f\"Lote {i}-{i+batch_size}: Falló la carga de algunos documentos.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en lote {i}-{i+batch_size}: {e}\")\n",
    "            errores.append(f\"Error en lote {i}-{i+batch_size}: {e}\")\n",
    "            break\n",
    "\n",
    "    return errores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = json.load(open(\"/app/index_data/data_processed_embeddings/step5_combined_2.json\"))\n",
    "\n",
    "errores = upload_to_azure_search(datos, index_name, search_endpoint, search_api_key)\n",
    "if errores:\n",
    "    logging.error(f\"Errores en la carga a Azure Search: {errores}\")\n",
    "else:\n",
    "    logging.info(\"Carga a Azure Search completada exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
